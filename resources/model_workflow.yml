# Model Workflow Configuration
# This file defines the workflow for training and optimizing the Author DSPy Model

resources:
  jobs:
    author_dspy_model:
      name: "author_dspy_model"
      description: "Training and optimization workflow for BMA Author DSPy Model using DSPy"
      
      tags:
        framework: "dspy"
      
      # Job configuration
      job_clusters:
        - job_cluster_key: "training_cluster" 
          new_cluster:
            spark_version: "16.4.x-cpu-ml-scala2.12"
            node_type_id: "Standard_D8s_v5"
            num_workers: 0
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
              "spark.databricks.delta.preview.enabled": "true"
              "spark.databricks.delta.retentionDurationCheck.enabled": "false" # usefull for testing but not production
            custom_tags:
              purpose: "ml_training"
              environment: "${var.environment}"
              ResourceClass: SingleNode
      
      # Task definitions
      tasks:
        - task_key: author_dspy_model
          # job_cluster_key: "training_cluster" #commenting for serverless
          # Dependencies are installed via %pip commands in the notebook itself
          notebook_task:
            notebook_path: ../src/author_model/author_model.py
            source: WORKSPACE
            base_parameters:
              # Environment parameters
              environment: "${var.environment}"
              catalog_name: "test_catalog"
              schema_name: "test_schema"
              
              # Model and experiment configuration
              experiment_name: "${var.experiment_name}"
              model_name: "bma_dspy_model"
              
              # Endpoint configuration
              chat_endpoint_name: "databricks-claude-3-7-sonnet"
              small_chat_endpoint_name: "databricks-gpt-oss-20b"
              larger_chat_endpoint_name: "databricks-claude-3-7-sonnet"
              reflection_chat_endpoint_name: "databricks-claude-sonnet-4-5"
              
              # Vector search configuration
              vector_search_endpoint: "ctcbl-unstructured-endpoint"
              vector_index_name: "test_volume_ctcbl_chunked_index_element__v0_0_1"
              evaluation_dataset_table: "rag_eval_dataset"
              
              # Training configuration
              num_threads: "2"
              test_mode: "false"  # Set to "true" for quick testing (3 train/3 test), "false" for full data (70/30 split)
              reflection_minibatch_size: "5"
              
              # RAG configuration
              retriever_k: "5"
              vector_search_num_results: "5"
              reranker_enabled: "true"
              
              # Reproducibility and optimization control
              seed: "1"
              enable_miprov2: "true"
              enable_gepa: "true"
              
              # Conversation history configuration
              max_history_length: "10"
              enable_history: "true"
      
      # Schedule configuration (optional)
      #schedule:
      #  quartz_cron_expression: "0 0 6 * * ?"  # Daily at 6 AM
      #  timezone_id: "UTC"
      #  pause_status: "UNPAUSED"
      
      # Email notifications
      #email_notifications:
      #  on_start:
      #    - ""
      #  on_success:
      #    - ""
      #  on_failure:
      #    - ""
      
      # Timeout settings
      timeout_seconds: 5400  # 1.5 hours
      max_concurrent_runs: 3  # Allow up to 3 concurrent training runs
